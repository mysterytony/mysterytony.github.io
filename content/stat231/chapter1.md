# Chapter 1: Introduction to Statistical Science

## 1.1 Statistical Sciences

Statistical Science are concerned with all aspects of **empirical studies** including problem formulation, planning of an experiment, data collection, analysis of the data, and the conclusion that can be made. An empirical study is one in which we learn by observation or experiment. A key feature of such studies is that there is usually uncertainty in the conclusions. An important task in empirical studies is to quantify this uncertainty.

Empirical studies deal with **populations** and **processes**; both of which are collections of individual **units**. In order to increase our knowledge about a process, we examine a **sample** of units generated by the process. To study a population of units we examine a sample of units carefully selected from that population. Two challenges arise since we only see a sample from the process or population and not all of the units are the same. Statistical Sciences deal both with the study of variability in processes and populations, and with good (that is, informative, cost-effective) ways to collect and analyze data about such processes.

## 1.2 Collecting Data

A **population** is a collection of **units**. A **process** is a system by which units are produced. A key feature of processes is that they usually occur over time whereas populations are often static.

We pose questions about populations (or processes) by defining **variates** for the units which are characteristics of the units. Variates can be different types. Variates such as height and weight of a person, lifetime or an electrical component, and time until recurrence of disease after medical treatment are all examples of **continuous** or **measured** variates. Variates such as the number of defective smart phones sold by a particular company in a week, the number of deaths in a year on a dangerous highway or the number of damaged pixels in a monitor are all examples of **discrete** variates.

Variates such as hair color, university program or martial status are examples of **categorical** variates since these variates do not take on numerical values. Sometimes, to facilitate the analysis of the data, we might redefine the variate of intersect to be 1 if present and 0 if absent. In such a case we would now call the variate a discrete variate. Since the variate only takes on values 0 or 1 such a variate is also often called a **binary** variate.

If a variate classifies a unit by size then this is an example of a categorical variate. However since this categorical variate has a natural ordering, it is also called an **ordinal** variate. 

Statistical Sciences stress the importance of obtaining data that will be objective and provide maximal information at a reasonable cost. There are three broad approaches:

*	**Sample Surveys**. The object of many studies is to learn about a finite population. In this case information about the population may be obtained by selecting a "representative" sample of units from the population and determining the variate of interest for each unit in the sample. Obtaining such a sample can be challenging and expensive. In a survey sample the variates of interest are most often collected using a questionnaire. Sample survey are widely used in government statistical studies, economics, marketing, public opinion polls, sociology, quality assurance and other areas.
*	**Observational Studies**. An observational study is one in which data are collected about a process or population without any attempt to change the value of one or more variates for the sampled units. A distinction between a sample survey and an observational study is that for observational studies the population of interest is usually infinite or conceptual.
*	**Experiments or Experimental Studies**. An experiment is a study in which the experimenter intervenes and changes or sets the values of one or more variates for the units in the sample.

## 1.3 Data Summaries

There are two classes of summaries: graphical and numerical. Suppose that data on a variate $y$ is collected for $n$ units in a population or process. By convention, we label the units as $1,2,...,n$ and denote their respective $y$-value as $y_1,y_2,...,y_n$. We might also collect data on a second variate $x$ for each unit, and we would denote the values as $x_1,x_2,...,x_n$. We refer to $n$ as the **sample size** and to $\{x_1,x_2,...,x_n\}$, $\{y_1,y_2,...,y_n\}$ or $\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$ as data sets.

#### Numerical Summaries

These summaries fall generally into three categories: measures of location (mean, median, and mode), measure of variability or dispersion (variance, range, and interquartile range), and measures of shape (skewness and kurtosis).

#### 1. Measure of locations:

*	The **sample mean** also call the sample average: $\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_i$.
*	The **sample median** $\hat{m}$ or the middle value when $n$ is odd and the sample is ordered from smallest to largest, and the average of the two middle values when $n$ is even. Since the median is less affected by a few extreme observations it is a more robust measure of location.
*	The **sample mode**, or the value of $y$ which appears in the sample with the highest frequency (not necessarily unique).

#### 2. Measure of dispersion or variability:

*	The **sample variance**:
$$s^2=\frac{1}{n-1}\sum_{i=1}^{n}(y_i-\bar{y})^2=\frac{1}{n}\left[\sum_{n=1}^{n}y_i^2-n(\bar{y})^2\right]$$
and the **sample standard deviation**: $s=\sqrt{s^2}$.
*	The **range** $=y_{(n)}-y_{(1)}$ where $y_{(n)}=max(y_1,y_2,...,y_n)$ and $y_{(1)}=min(y_1,y_2,...,y_n)$.
*	The **interquartile range IQR** which is described below.

The sample variance and the sample standard deviation measure the variability or spread of the variate values in a data set. The units for standard deviation, range and interquartile range are the same as for the original variate.

#### 3. Measures of shape:

Measures of shape generally indicate how the data, in terms of a relative frequency histogram, differ from the Normal bell-shaped curve, for example whether one tail of the relative frequency histogram is substantially larger than the other so the histogram is asymmetric, or whether both tails of the relative frequency histogram are large so the data are more prone to extreme values than data from a Normal distribution.

*	The **sample skewness**
$$g_1=\frac{\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^3}{\left[\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^2\right]^{3/2}}$$

	is a measure of the (lack of) symmetry in the data.

	When the relative frequency histogram of the data is approximately symmetric then there is an approximately equal balance between the positive and negative values in the sum $\sum_{i=1}^{n}(y_i-\bar{y})^3$ and this results in a value for the skewness that is approximately zero. 

	If the relative frequency histogram of the data has a long right tail, then the positive values of $(y_i-\bar{y})^3$ dominate the negative values in the sum and the value of the skewness will be positive. 

	Similarly if the relative frequency histogram of the data has a long left tail then the value of the skewness will be negative.

*	The **sample kurtosis**
$$g_2=\frac{\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^4}{\left[\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^2\right]^2}$$

	measures the heaviness of the tails and the peakedness of the data relative to data that are Normally distributed. For the Normal distribution the kurtosis is equal to 3.

	Since the term $(y_i-\bar{y})^4$ is always positive, the kurtosis is always positive and values greater than three indicate heaver tails (and a more peaked center) than data that are Normally distributed.

#### Sample Quantiles and Percentiles

For $0<p<1$, the $p$-th quantile (also called the $100p$-th percentile) is a value such that approximately a fraction $p$ of the $y$ values in the data set are less than $q(p)$ and roughly $1-p$ are greater. Depending on the size of the data set, quantiles are not uniquely defined for all values of $p$. There are different conventions for defining quantiles in these cases. If the sample size is large, the differences in the quantiles based on the various definitions are small.

#### Definition 1

Let $\{y_{(1)},...,y_{(n)}\}$ where $y_{(1)}\leq\cdots\leq y_{(n)}$ be the order statistic for the data set $\{y_1,...,y_n\}$. The $p$-th **sample quantile** is a value, call it $q(p)$, determined as follows:

*	Let $m=(n+1)p$ where $n$ is the sample size
*	If $m$ is an integer between $1$ and $n$ then $q(p)=y_{(m)}$ which is the $m$-th largest value in the data set.
*	If $m$ is not an integer but $1<m<n$ when determine the closest integer $j$ such that $j<m<j+1$ and take $q(p)=\frac{1}{2}[y_{(j)}+y_{(j+1)}]$.

The quantiles $q(0.25)$, $q(0.5)$, $q(0.75)$ are often used to summarize a data set and are given special names.

#### Definition 2

The **quantiles** $q(0.25)$, $q(0.5)$, $q(0.75)$ are called the lower or first quartile, the median, and the upper or third quartile respectively.

#### Definition 3

The **interquartile range is IQR** $=q(0.75)-q(0.25)$.

Since the interquartile range is less affected by a few extreme observations, it is a more robust measure of variability as compared to the sample standard deviation.

#### Definition 4

The **five number summary** of a data set consists of the smallest observation, the lower quartile, the median, the upper quartile and the largest value, that is, the five values $y_{(1)}, q(0.25), q(0.5), q(0.75), y_{(n)}$.

The five number summary provides a concise numerical summary of a data set which provides information about the location (through the median), the spread (through the lower and upper quartiles) and the range (through the minimum and maximum values).

#### Sample Correlation

So far we have looked only at graphical summaries of a data set. Often we have bivariate data of the form $\{(x_1,y_1),...,(x_n,y_n)\}$. A numerical summary of such data is the sample correlation.

#### Definition 5

The **sample correlation**, denoted by $r$ is
$$r=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}$$
where
$$S_{xx}=\sum_{i=1}^{n}(x_i-\bar{x})^2=\sum_{i=1}^{n}x_i^2-n(\bar{x})^2$$
$$S_{xy}=\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^{n}x_iy_i-n\bar{x}\bar{y}$$
and
$$S_{yy}=\sum_{i=1}^{n}(y_i-\bar{y})^2=\sum_{i=1}^{n}y_i^2-n(\bar{y})^2$$

The sample correlation, which takes on values between $-1$ and $1$, is a measure of the linear relationship between the two variate $x$ and $y$. If the value of $r$ is close to 1 then we say that there is a strong positive linear relationship between the two variates while if the value of $r$ is close to $-1$ then we say that there is a strong negative linear relationship between the two variates. If the value of $r$ is close to $0$ then we say that there is no linear relationship between the two variates.

#### Relative Risk

Recall that categorical variates consist of group of category names that do not necessary have any ordering. If two variates of interest in a study are categorical variates then it does not make sense to use sample correlation as a measure of the relationship between the two variates.

#### Definition 6

For categorical data, the **relative risk** of event $A$ in group $B$ as compared to group $\bar{B}$ is
$$relative \quad risk = \frac{y_{11}/(y_{11}+y_{12})}{y_{21}/(y_{21}+y_{22})}$$

#### Graphical Summaries

We consider several types of plots for a data set $\{y_1,...,y_2\}$ and one type of plot for a data set $\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$.

#### Frequency histogram

Consider measurements $\{y_1,...,y_2\}$ on a variate $y$. Partition the range of $y$ into $k$ non-overlapping intervals $I_j=[a_{j-1},a_j), j=1,2,...,k$ and then calculate for $j=1,2,...,k$. $f_j=$ number of values from $\{y_1,...,y_2\}$ that are in $I_j$.

The $f_j$ are called the **observed frequencies** for $I_1,...,I_k$; note that $\sum_{j=1}^{k} k_j=n$. A **histogram** is a graph in which a rectangle is placed above each interval; the height of the rectangle for $I_j$ is chosen so that the area of the rectangle is proportional to $f_j$. Two main types of frequency histogram are:

1.	a "standard" frequency histogram where the intervals $I_j$ are of equal length. The height of the rectangle for $I_j$ is the frequency $f_j$ or **relative frequency** $f_j/n$. This type of histogram is similar to a bar chart.
2.	a "relative" frequency histogram, where the interval $I_j=[a_{j-1},a_j)$ may or may not be of equal length. The height of the rectangle for $I_j$ is chosen so that its area equals $f_j/n$, that is, the height of the rectangle for $I_j$ is equal to
$$\frac{f_j/n}{(a_j-a_{j-1})}$$

If intervals of equal length are used then a standard frequency histogram and a relative frequency histogram look identical except for the labeling of the vertical axis. For a relative frequency histogram the sum of the areas of the rectangles is equal to one. If we wish to compare two data sets which have different sample sizes then a relative frequency histogram must be used. If we wish to superimpose a probability density function on a frequency histogram to see how well the data fit the model then a relative frequency histogram must always be used.

To construct a frequency histogram, the number and location of the intervals must be chosen. The intervals are typically selected so that there are ten to fifteen intervals and each interval contains at least one $y$-value from the sample.

#### Empirical Cumulative Distribution Functions

Another way to portray the values of a variate $\{y_1,...,y_2\}$ is to determine the proportion of values in the set which are smaller than any given value. This is called the **empirical cumulative distribution function** or e.c.d.f. and is defined by
$$\hat{F}(y)=\frac{number \quad of \quad values \quad in \quad \{y_1,...,y_2\} \quad which \quad are \quad \leq y}{n}$$

To construct $\hat{F}(y)$, it is convenient to first order the $y_i$'s to give the ordered values. Then we note that $\hat{F}(y)$ is a step function with a jump at each of the ordered observed values.

#### Boxplots

In many situations, we want to compare the values of a variate for two or more groups. Especially when the number of groups is large (or the sample sizes within groups are small), side-by-side boxplot are a convenient way to display the data. Boxplots are also called box and whisker plots.

The boxplot is usually displayed vertically. The line inside the box corresponds to the median and the bottom and top sides of the box correspond to the lower quartile $q(0.25)$ and the upper quartile $q(0.75)$ respectively. The so-called whiskers extend down and up from the box to a horizontal line. The lower line is placed at the smallest data value that is larger than the value $q(0.25)-1.5\times IQR$ where $IQR=q(0.75)-q(0.25)$ is the interquartile range. Similarly the upper line is placed at the largest observed data value that is smaller than the value $q(0.75)+1.5IQR$. Any values beyond the whiskers (often called outliers) are plotted with special symbols.

Boxplots are particularly used for comparing several groups. The graphical summaries we have just discussed are most useful for summarizing variates which are either continuous or discrete with many possible values. For a categorical variates the data can be best summarized using bar graphs and pie charts. Such graphs can be used incorrectly.

The graphical summaries discussed to this point deal with a single variate. If we have data on two variate $x$ and $y$ for each unit in the sample then data set is represented as $\{(x_i, y_j), i=1,2,...,n\}$. We often interested in examining the relationships between the two variates.

#### Scatterplots

A scaterplot, which is a plot of the points $(x_i, y_j), i=1,2,...,n$, can be used to see whether the two variates are related in some way.

## 1.4 Probability Distributions and Statistical Models

Statistical models are used to describe processes such as the daily closing value of a stock or the occurrence and size of claims over time in a portfolio of insurance policies. With populations, we use a statistical model to describe the selection of the units and the measurement of the variates. The model depends on the distribution of variate values in the population (that is, the population histogram) and the selection procedure. We exploit this connection when we want to estimate attributes of the population and quantify the uncertainty in our conclusions. We use the models in several ways:

*	questions are often formulated in terms of parameters of the model
*	the variate values vary so random variables can describe the variation
*	empirical studies usually lead to inferences that involve some degree of uncertainty, and probability is used to quantify this uncertainty
*	procedures for making decisions are often formulated in terms of models
*	models allow us to characterize processes and to simulate them via computer experiments

#### Example 1.4.1 A Binomial Distribution Example

Consider again the survey of smoking habits of teenagers described in Example 1.2.1. To select a sample of 500 units (teenagers living in Ontario), suppose we had a list of most of the units in the population. Getting such a list would be expensive and time consuming so the actual selection procedure is likely to be very different. We select a sample of 500 units from the list at random and count the number of smokers in the sample. We model this selection process using a Binomial random variable $Y$ with probability function (p.f.)

$$P(Y=y;\theta)={500 \choose y}\theta^y (1-\theta)^{500-y}$$ for $y=0,1,...,500$

Here the parameter $\theta$ represents the unknown proportion of smokers in the population, one attribute of interest in the study.

#### Example 1.4.2 An Exponential Distribution Example

In Example 1.3.4, we examined the lifetime (in 1000 km) of a sample of 200 front brake pads taken from the population of all cars of a particular model produced in a given time period. We can model the lifetime of a single brake pad by a continuous random variable $Y$ with Exponential probability density function (p.d.f.)

$$f(y;\theta)=\frac{1}{\theta} e^{-y/\theta}$$ for $y>0$

There the parameter $\theta > 0$ represents the mean lifetime of the brake pads in the population since, in the model, the expected value of $Y$ is $E(Y)=\theta$.

To model the sampling procedure, we assume that he date $\{y_1,y_2,...,y_200\}$ represent 200 independent realizations of the random variable $Y$. That is, we let $Y_i=$ the lifetime for the $i$th brake pad int eh sample, $i=1,2,...,200$ and we assume that $Y_1,...,Y_{200}$ are independent Exponential random variables each having the same mean $\theta$.

We can use the model and the data to estimate $\theta$ and other attributes of interest such as the proportion of brake pads that fail in the first 100,000 km of use. In terms of the model, we can represent this proportion by

$$P(Y\leq 100; \theta)=\int_0^{100}f(y;\theta)dy=1-e^{-100/\theta}$$

If we model the selection of a data set $\{y_1,...,y_n\}$ as $n$ independent realizations of a random variable $Y$ as in the above brake pad example, we can draw strong parallels between summaries of the data set described in Section 1.3 and properties of the corresponding probability model $Y$. For example,

*	The sample mean $\bar{y}$ corresponds to the population mean $E(Y)=\mu$
*	The sample median $\hat{m}$ corresponds to the population median $m$. For continuous distributions the population median is the solution $m$ of the equation $F(m)=0.5$ where $F(y)=P(Y\leq y)$ is the cumulative distribution function of $Y$. For discrete distributions, it is a point $m$ chosen such that $P(Y\leq m) \geq 0.5$ and $P(Y\geq m)\geq 0.5$.
*	The sample standard deviation $s$ corresponds to $\sigma$, the population standard deviation of $Y$, where $\sigma^2=E\[(Y-\mu)^2\]$.
*	The relative frequency histogram corresponds to the probability histogram of $Y$ for discrete distributions and the probability density function of $Y$ for continuous distributions.

#### Example 1.4.3 A Gaussian Distribution Example

Earlier, we described an experiment where the goal was to see if there is a relationship operating performance $y$ of a computer chip and ambient temperature $x$. In the experiment, there were four groups of 10 chips and each group operated at a different temperature $x=10,20,30,40$. The data are $\{(x_1,y_1),...,(x_{40},y_{40})\}$. A model for $Y_1,...,Y_{40}$ should depend on the temperature $x_i$ and one possibility is to assume $Y_i\sim G(\beta_0+\beta_1x_i,\sigma), i=1,2,...,40$ independently. In this model, the mean of $Y$ is a linear function of the temperature $x_i$. The parameter $\sigma$ allows for variability in performance among chip operating at the same temperature. We will consider such models in detail in Chapter 6.

#### Response versus Explanatory Variates

Suppose we wanted to study the relationship between second hand smoke and asthma among children aged 10 and under. The two variates of interest could be defined as:

$x=$ whether the child lives in a household where adults smoke,

$y=$ whether the child suffers from asthma.

In this study there is a natural division of the variates into two types: response variate and explanatory variate. In this example, $Y$, the asthma status, is the response variate (often coded as $Y=1$ if child suffers from asthma, $Y=0$ otherwise) and $x$, whether the child lives in a household where adults smoke, is explanatory variate (also often coded as $x=1$ if child lives in a household where adults smoke and $x=0$ otherwise). The explanatory variate $x$ is in the study of partially explain or determine the distribution of the response variate.

Similarly in an observational study of 1718 men aged 40-45, the men were classified according to whether they were heavy coffee drinkers (more than 100 cups/month) or not (less than 100 cups/month) and whether they suffer from CHD (coronary heart disease) or not. The question of interest is whether there is a relationship between coffee consumption and CHD. Unlike Example 1.4.3, neither variate is under the control of the researchers. We might be interested in whether coffee consumption can be used to "explain" CHD. In this case we would call coffee consumption as explanatory variate while CHD would the response variate. However if we were interested in whether CHD can be used to explain coffee consumption then CHD would be the explanatory variate and coffee habits would be the response variate.

In some cases it is not clear which is the explanatory variate and which is the response variate. For example, the response variate $Y$ might be the weight of a randomly selected female in the age range 16-25, in some population. A person's weight is related to their height. We might want to study this relationship by considering females with a given height $x$ and proposing that distribution of $Y$, given $x$ is Gaussian, $G(\alpha + \beta x, \sigma)$. That is, we propose that the average weight of a female depends linearly on her height $x$ and write this as

$$E(Y|x)=\alpha +\beta x$$

However it would be possible to reverse the roles of the two variates here and consider the weight to be an explanatory variate and height the response variate, if for example we wished to predict height using data on individuals' weights.

## 1.5 Data Analysis and Statistical Inference

Whether we are collecting data to increase our knowledge or to serve as a basis for making decisions, proper analysis of the data is crucial. We distinguish between two broad aspects of the analysis and interpretation of data. The first is what we refer to as **descriptive statistics**. This is the portrayal of the data, or parts of it, in numerical and graphical ways so as to show feature of interest.  We have considered a few methods of descriptive statistics in Section 1.3. The terms data mining and knowledge discovery in data bases (KDD) refer to exploratory data analysis where the emphasis is on descriptive statistics. This is often carried out on very large data bases. The goal, often vaguely specified, is to find interesting patterns and relationships.

A second aspect of a statistical analysis of data is what we refer to as **statistical inference**. That is, we use the data obtained in the study of a process or population to draw general conclusions about the process or population itself. That is a form of inductive inference, in when we reason from the specific (the observed data on a sample of units) to the general (the target population or process). This may be contrasted with deductive inference (as in  logic and mathematics) in which we use general results to prove specific things.

This course introduces some basic methods of statistical inference. Three main types of program will be discussed, loosely referred to as **estimation problem**, **hypothesis testing problems** and **prediction problems***. In the first type, the problem is to estimate one or more attributes of a process or population. For example, we may wish to estimate the proportion of Ontario residents aged 14 - 20 who smoke, or to estimate the distribution of survival times for certain types of AIDS patients. Another type of estimation problem is that of "fitting" or selecting a probability model for a process.

Hypothesis testing problems involve using the data to assess the truth of some question of hypothesis. For example, we may hypothesis that in the 14-20 age group a higher proportion of females than males smoke, or that the use of a new treatment will increase the average survival time of AIDS patients by at least 50 percent.

In prediction problems, we use the data to predict a future value for a process variate or a unit to be selected from the population. For example, based on the results of a clinical trial such as Example 1.2.3, we may wish to predict how much an individual's blood pressure would drop for a given dosage of a new drug. Or, given the past performance of a stock and other data, to predict the value of the stock at some point in the future.

Statistical analysis involves the use of both descriptive statistics and formal methods of estimation, prediction and hypothesis testing.

#### Example 1.5.1 A Smoking behavior survey

Suppose in Example 1.2.1, we sampled 250 males and 250 females aged 14-20 as described in Example 1.4.1. Here we focus only on the sex of each person in the sample, and whether or not they smoked. The data are summarized in the following two-way table:

| | Smoker | Non-smoker | Total |
|---|---|---|---|
| Female | 82 | 168 | 250 |
| Male | 71 | 179 | 250 |
| Total | 153 | 347 | 500 |

Suppose we are interested in the question "Is the smoking rate among teenager girls higher than the rate among teenager boys?" From the data, we see that the sample proportion of girls who smoke is $82/250=0.328$ and the sample proportion of males who smoke is $71/250=0.284$. In the sample, the smoking rate for females is higher. But what can we say about the who population? To proceed, we formulate the hypothesis that there is no difference in the population rates. then assuming the hypothesis is true, we construct two Binomial models as in Example 1.4.1 each with a common parameter $\theta$. We can estimate $\theta$ using the combined data so that $\hat{\theta}=153/500=0.306$. Then using the model and the estimate, we can calculate the probability of such a large difference in the observed rates. Such a large difference occurs about $20\%$ of the time so such a large difference is observed rates happens fairly often and therefore, based on the observed data, there is no evidence of a difference in the population smoking rates.
